summary(myLm3) # Note p-value for cubic term (it's small)
#linear model (lm)
summary(myLm3)
plot(myLm3,which=1)
summary(myLm3) # Note p-value for cubic term (it's small)
height4 <- Height^4
myLm4 <- update(myLm3, ~ . + height4)
summary(myLm4) # Note p-value for quartic term (it's not small)
myLm2 <- lm(Distance ~ Height + height2)
summary(myLm2)
plot(myLm2, which=1)
summary(myLm2) # Note p-value for quadratic term (it's small)
height3 <- Height^3
myLm3 <- update(myLm2, ~ . + height3)  #update adds another variable to the
wd()
wd
cwd()
?cwd()
?cwd
?wd
library(Sleuth3)
#linear model (lm)
summary(myLm3)
attach(case1001)
?case1001
attach(case1001)
?case1001
library('Sleuth3')
case1001
attach(case1001)
?case1001
## EXPLORATION
?plot
plot(Distance ~ Height)
myLm <- lm(Distance ~ Height)
abline(myLm)
summary(myLm)
plot(myLm, which=1)
height2 <- Height^2
myLm2 <- lm(Distance ~ Height + height2)
summary(myLm2)
plot(myLm2, which=1)
summary(myLm2) # Note p-value for quadratic term (it's small)
height3 <- Height^3
myLm3 <- update(myLm2, ~ . + height3)  #update adds another variable to the
#linear model (lm)
summary(myLm3)
plot(myLm3,which=1)
summary(myLm3) # Note p-value for cubic term (it's small)
height4 <- Height^4
myLm4 <- update(myLm3, ~ . + height4)
summary(myLm4) # Note p-value for quartic term (it's not small)
UltimateLM = lm(Distance ~ poly(Height,5)) #adds polynomial values to it
# NaN in the residuals means that there is more polynomials then data points
summary(UltimateLM)
plot(Distance ~ Height, xlab="Initial Height (Punti)",
ylab="Horizontal Distance Traveled (Punti)",
main="Galileo's Falling Body Experiment",
pch=21, bg="red", lwd=2, cex=2)
#lines(Height, myLm4$fitted.values[order(Height)])
lines(Height, UltimateLM$fitted.values[order(Height)])
## DISPLAY FOR PRESENTATION
plot(Distance ~ Height, xlab="Initial Height (Punti)",
ylab="Horizontal Distance Traveled (Punti)",
main="Galileo's Falling Body Experiment",
pch=21, bg="red", lwd=2, cex=2)
dummyHeight     <- seq(min(Height),max(Height),length=100)
betaQ           <- myLm2$coef
quadraticCurve  <- betaQ[1] + betaQ[2]*dummyHeight + betaQ[3]*dummyHeight^2
lines(quadraticCurve ~ dummyHeight,col="blue",lwd=3)
betaC           <- myLm3$coef # coefficients of cubic model
cubicCurve      <- betaC[1] + betaC[2]*dummyHeight + betaC[3]*dummyHeight^2 +
betaC[4]*dummyHeight^3
lines(cubicCurve ~ dummyHeight,lty=3,col="red",lwd=3)
legend(590,290,legend=c(expression("Quadratic Fit  "*R^2*" = 99.0%"),
expression("Cubic Fit        "*R^2*" = 99.9%")),
lty=c(1,3),col=c("blue","red"), lwd=c(3,3))
detach(case1001)
x = seq(0,1,.1)
error = rnorm(length(x),0,.05)
true_y = x*(x-1)
obs_y = true_y + error
plot(x,obs_y, col = 'red')
lines(x,true_y,type = 'l')
model = lm(obs_y~poly(x,2))
summary(model)
plot(model)
RSS=c()
for (i in c(1:5)) {
model = lm(obs_y~poly(x,i))
lines(x,model$fitted.values,type = 'l', col='blue')
RSS = c(RSS,sum(model$residuals^2))
}
RSS
plot(c(1:5),RSS,type = 'l',xlab="degree of polynomial")
shapiro.test(model$residuals)
x = seq(0,1,.1)
error = rnorm(length(x),0,.05)
# this random error will help mess with generating function
true_y = x*(x-1)
obs_y = true_y + error
plot(x,obs_y, col = 'red')
lines(x,true_y,type = 'l')
model = lm(obs_y~poly(x,7))
summary(model)
plot(model)
x = seq(0,1,.1)
error = rnorm(length(x),0,.05)
true_y = x*(x-1)
obs_y = true_y + error
plot(x,obs_y, col = 'red')
?seq
plot(x,obs_y, col = 'red')
plot(x,obs_y, col = 'red')
obs_y = true_y + error
x = seq(0,1,.1)
error = rnorm(length(x),0,.05)
true_y = x*(x-1)
obs_y = true_y + error
plot(x,obs_y, col = 'red')
lines(x,true_y,type = 'l')
model = lm(obs_y~poly(x,2))
summary(model)
plot(model)
RSS=c()
for (i in c(1:5)) {
model = lm(obs_y~poly(x,i))
lines(x,model$fitted.values,type = 'l', col='blue')
RSS = c(RSS,sum(model$residuals^2))
}
RSS
plot(c(1:5),RSS,type = 'l',xlab="degree of polynomial")
shapiro.test(model$residuals)
mtcars
attach(mtcars)
library(ggplot2)
ggplot(data=mtcars) + geom_point(mapping = aes(log(disp), log(mpg)))
model1 <- lm(disp~poly(mpg,1))
model1 <- lm(disp~poly(mpg,1))
summary(model1)
model3 <- lm(log(disp) ~log(mpg))
model1 <- lm(disp~poly(mtcars$mpg,1))
summary(model1)
plot(model1)
shapiro.test(model1$residuals)
model3 <- lm(log(disp) ~log(mpg))
model3 <- lm(log(disp) ~log(mtcars$mpg))
summary(model3)
plot(model3)
a
ggplot(data=mtcars) + geom_point(mapping = aes(log(disp), log(mpg)))
shapiro.test(model3$residuals)
#p1 + scale_y_sqrt()
#p4 <- p1 + scale_x_log10()
#p4
ggplot(data=mtcars) + geom_point(mapping = aes(disp, mpg))
model2 <- lm(disp~poly(mpg,2))
model2 <- lm(disp~poly(mtcars$mpg,2))
summary(model2)
plot(model2)
shapiro.test(model2$residuals)
ggplot(data=mtcars) + geom_point(mapping = aes(sqrt(disp), sqrt(mpg)))
model2 <- lm(disp~sqrt(mtcars$mpg))
summary(model2)
plot(model2)
shapiro.test(model2$residuals)
ggplot(data=mtcars) + geom_point(mapping = aes(disp, sqrt(mpg)))
ggplot(data=mtcars) + geom_point(mapping = aes(sqrt(disp), mpg))
model2 <- lm(sqrt(disp)~mtcars$mpg)
summary(model2)
plot(model2)
shapiro.test(model2$residuals)
model2 <- lm((disp)^(-2)~mtcars$mpg)
summary(model2)
plot(model2)
shapiro.test(model2$residuals)
model2 <- lm((disp)^(-1)~mtcars$mpg)
summary(model2)
plot(model2)
shapiro.test(model2$residuals)
model3 <- lm(log(mtcars$mpg) ~log(disp))
summary(model3)
plot(model3)
shapiro.test(model3$residuals)
model1 <- lm(mtcars$mpg~poly(disp,1))
summary(model1)
plot(model1)
shapiro.test(model1$residuals)
model2 <- lm((mtcars$mpg)^(-1/2)~disp)
summary(model2)
plot(model2)
shapiro.test(model2$residuals)
model1 <- lm(mtcars$mpg~disp)
summary(model1)
plot(model1)
shapiro.test(model1$residuals)
#p1 + scale_y_sqrt()
#p4 <- p1 + scale_x_log10()
#p4
ggplot(data=mtcars) + geom_point(mapping = aes(disp, mpg))
ggplot(data=mtcars) + geom_point(mapping = aes(disp^(-1/2), mpg))
model2 <- lm(mtcars$mpg~disp^(-1/2))
summary(model2)
plot(model2)
shapiro.test(model2$residuals)
model2 <- lm(mtcars$mpg~disp^(-1/2))
model2 <- lm(mtcars$mpg~(disp^(-1/2)))
model2 <- lm(mtcars$mpg~inverse.sqrt(disp))
install.packages('hglm')
model2 <- lm(mtcars$mpg~inverse.sqrt(disp))
#install.packages('hglm')
library('hglm')
model2 <- lm(mtcars$mpg~inverse.sqrt(disp))
?inverse.sqrt
model2 <- lm(mtcars$mpg~disp.inverse.sqrt())
inv <- disp +inverse.sqrt()
inv <- inverse.sqrt(disp)
model1 <- lm(log(mtcars$mpg)~disp)
summary(model1)
plot(model1)
#install.packages('hglm')
#library('hglm')
shapiro.test(model1$residuals)
ggplot(data=mtcars) + geom_point(mapping = aes(disp, log(mpg)))
(mapping = aes(disp^(-1/2), mpg))
ggplot(data=mtcars) + geom_point(mapping = aes(disp^(-1/2), mpg))
abline(model1)
model2 <- lm(mtcars$mpg~inverse(sqrt(disp)))
?inverse()
?sqrt()
?^
model2 <- lm(mtcars$mpg~(1/sqrt(disp)))
summary(model2)
plot(model2)
shapiro.test(model2$residuals)
model2 <- lm(mtcars$mpg~reciporcal(sqrt(disp)))
model2 <- lm(mtcars$mpg~reciprocal(sqrt(disp)))
model2 <- lm(mtcars$mpg~log(disp))
summary(model2)
plot(model2)
shapiro.test(model2$residuals)
model3 <- lm(log(mtcars$mpg)~log(disp))
summary(model3)
plot(model3)
model3 <- lm(log(mtcars$mpg)~disp)
summary(model3)
plot(model3)
shapiro.test(model3$residuals)
ggplot(data=mtcars) + geom_point(mapping = aes(disp, sqrt(mpg)))
model2 <- lm(sqrt(mtcars$mpg)~(disp))
summary(model2)
plot(model2)
shapiro.test(model2$residuals)
model2 <- lm(mtcars$mpg~sqrt(disp))
summary(model2)
plot(model2)
shapiro.test(model2$residuals)
model2 <- lm(mtcars$mpg~poly(disp,2))
summary(model2)
plot(model2)
shapiro.test(model2$residuals)
histData <- c(mtcars$mpg, disp)
hist(histData)
histData1 <- c(mtcars$mpg, log(disp))
hist1(histData)
hist(histData1)
histData2 <- c(mtcars$mpg, ln(disp))
histData2 <- c(mtcars$mpg, logb(disp,e))
histData2 <- c(mtcars$mpg, exp(disp)
histData2 <- c(mtcars$mpg, exp(disp))
histData2 <- c(mtcars$mpg, exp(disp))
hist(histData2)
histData2 <- c(mtcars$mpg, exp(disp + 1))
hist(histData2)
histData3 <- c(mtcars$mpg, sqrt(disp))
hist(histData2)
hist(histData3)
model3 <- lm(mtcars$mpg~sqrt(disp))
summary(model3)
plot(model3)
shapiro.test(model3$residuals)
histData3 <- c(mtcars$mpg, (1/sqrt(disp)))
hist(histData3)
hist(disp)
hist(mtcars$mpg)
hist(exp(disp))
hist(sqrt(disp))
disp.head(50)
head(disp,50)
summary(disp)
plot(disp)
histData1 <- c(mtcars$mpg, disp)
hist(histData1)
histData2 <- c(log(mtcars$mpg), log(disp))
hist(histData2)
histData2 <- c(mtcars$mpg, log(disp))
hist(histData2)
histData3 <- c(mtcars$mpg, sqrt(disp))
hist(histData3)
histData3 <- c(sqrt(mtcars$mpg), sqrt(disp))
hist(histData3)
histData3 <- c(sqrt(mtcars$mpg), disp)
hist(histData3)
histData3 <- c(mtcars$mpg, sqrt(disp))
hist(histData3)
model3 <- lm(mtcars$mpg~sqrt(disp))
summary(model3)
plot(model3)
shapiro.test(model3$residuals)
ggplot(data=mtcars) + geom_point(mapping = aes(disp, sqrt(mpg)))
ggplot(data=mtcars) + geom_point(mapping = aes(sqrt(disp), mpg))
ggplot(data=mtcars) + geom_point(mapping = aes(sqrt(disp), mpg)) + geom_abline((aes(intercept=38.8324, slope = -1.2809)))
x = seq(0,1,.1)
error = rnorm(length(x),0,.05)
true_y = x*(x-1)
obs_y = true_y + error
plot(x,obs_y, col = 'red')
lines(x,true_y,type = 'l')
model = lm(obs_y~poly(x,2))
summary(model)
x = seq(0,1,.1)
error = rnorm(length(x),0,.05)
true_y = x*(x-1)
obs_y = true_y + error
plot(x,obs_y, col = 'red')
lines(x,true_y,type = 'l')
model = lm(obs_y~poly(x,2))
summary(model)
plot(model)
true_y = x*(x-1)
obs_y = true_y + error
plot(x,obs_y, col = 'red')
lines(x,true_y,type = 'l')
model = lm(obs_y~poly(x,2))
summary(model)
# Graeme Beatie In class assignment 9/16
library(Ecdat)
data(University)
attach(University)
# make the train and test sets
train = sample(c(1:nrow(University)), .7*nrow(University), replace = FALSE)
#training data
training_data <- University[train,]
#test data
test_data = University[-train]
# creation of model 1
model1= lm(undstudents~(I(resgr^.5)+acnumbers+secrpay+agresrk), data = University, subset =train)
# creation of model 2
model2 = lm(undstudents~I(resgr^.5)+clernum+acpay, data = University, subset = train)
# Training RSS
sum((undstudents[train]- model1$fitted.values)^2)
# Testing RSS
sum((undstudents[train]-model2$fitted.values)^2)
# Comment on which model you prefer
# From the seed that I ran the RSS of the first model was considerably lower
# than the RSS of the second model. So I would choose to use the first one
# if I was just going off of testing RSS of the models.
train = read.csv("training.csv", header = TRUE)
setwd("~/R_scripts/Logistic Regression")
train = read.csv("training.csv", header = TRUE)
test = read.csv("testing.csv", header = TRUE)
head(train)
attach(train)
boxplot(TRAN_AMT~FRAUD_NONFRAUD)#This seems like a good indicator of fraud!
#Would a transform help?
par(mfrow=c(1,2))
boxplot((TRAN_AMT)~FRAUD_NONFRAUD, main= 'No transform')
boxplot(log(TRAN_AMT)~FRAUD_NONFRAUD, main= 'Log transform')
#Lets explor another variable
boxplot( log(ACCT_PRE_TRAN_AVAIL_BAL+1) ~ FRAUD_NONFRAUD,main= 'Log')
boxplot( (ACCT_PRE_TRAN_AVAIL_BAL) ~ FRAUD_NONFRAUD,main= 'None')
boxplot( log(WF_dvc_age+1) ~ FRAUD_NONFRAUD)
#model creation and evaluation
Glm1 <- glm(as.factor(FRAUD_NONFRAUD) ~ log(TRAN_AMT), data = train, family = binomial )
summary(Glm1)
#Predicted probabilities for test data
Glm1.probs = predict(Glm1, newdata =  test , type = "response")
#Classification rule & predicted class
glm1.pred = ifelse(Glm1.probs >.5 ,"Non-Fraud","Fraud")
#Confusion matrix
t = table(glm1.pred, test$FRAUD_NONFRAUD)
print(t)
(t[1,1]+t[2,2])/ sum(t)
sum(glm1.pred == test$FRAUD_NONFRAUD)/ sum(t) #Another way  to calculate accuracy
t[1,1]/(t[1,1]+t[2,1]) #Recall
t[2,2]/(t[2,2]+t[1,2])#Specificity
t[1,1]/(t[1,1]+t[1,2])#Precision
#ROC curve
library(pROC)
test_roc = roc(as.factor(test$FRAUD_NONFRAUD) ~ Glm1.probs, plot = TRUE, print.auc = TRUE)
# the best reasonable range for this ROC curve is right at that corner
# so looking at the curve we want sensitivity and spec to be atleast .7
# based off of the values from the curve around the corner
names(test_roc)
#Obtain new cut off value
cutoff = test_roc$thresholds[test_roc$sensitivities>.7 & test_roc$specificities >.7]
range(cutoff)
# this does the same thing but as a dataframe
roc.df = data.frame(sensitivities = test_roc$sensitivities,
specificities = test_roc$specificities,
thresholds = test_roc$thresholds)
head(roc.df)
cutoff = roc.df[roc.df$sensitivities>.7 & roc.df$specificities > .7,]
# this gives a good range for the cutoff
# go for the upper lower and middle value to see what is the best
range(cutoff$thresholds)
#What if we changed the criteria for classification
#Classification rule 2
glm1.pred2 = ifelse(Glm1.probs >.6 ,"Non-Fraud","Fraud")
#Confusion matrix
t2 = table(glm1.pred2, test$FRAUD_NONFRAUD)
print(t2)
(t2[1,1]+t2[2,2])/ sum(t2)
t2[1,1]/(t2[1,1]+t2[2,1]) #Recall
t2[2,2]/(t2[2,2]+t2[1,2])#Specificity
t2[1,1]/(t2[1,1]+t2[1,2])#Precision
install.packages('MASS')
library('MASS')
#Pre processing training and testing
data.train = data.frame(train$FRAUD_NONFRAUD,log(train$TRAN_AMT), log(train$ACCT_PRE_TRAN_AVAIL_BAL+1))
data.train[,unlist(lapply(data.train, is.numeric), use.names = FALSE)] = scale(data.train[,unlist(lapply(data.train, is.numeric), use.names = FALSE)  ] )
names(data.train) = c('FRAUD_NONFRAUD', "log.TRAN_AMT", "log.ACCT_PRE_TRAN_AVAIL_BAL")
data.test = data.frame(test$FRAUD_NONFRAUD,log(test$TRAN_AMT), log(test$ACCT_PRE_TRAN_AVAIL_BAL+1))
data.test[,unlist(lapply(data.test, is.numeric), use.names = FALSE)] = scale(data.test[,unlist(lapply(data.test, is.numeric), use.names = FALSE)  ] )
names(data.test) = c('FRAUD_NONFRAUD', "log.TRAN_AMT", "log.ACCT_PRE_TRAN_AVAIL_BAL")
lda.fit = lda(FRAUD_NONFRAUD~ log.TRAN_AMT + log.ACCT_PRE_TRAN_AVAIL_BAL, data = data.train)
lda.fit
plot(lda.fit)
install.packages('klaR')
library(klaR)
partimat(as.factor(FRAUD_NONFRAUD) ~ log.TRAN_AMT + log.ACCT_PRE_TRAN_AVAIL_BAL, data = data.train, method="lda")
partimat(as.factor(FRAUD_NONFRAUD)~ log(TRAN_AMT) + log(1+ACCT_PRE_TRAN_AVAIL_BAL), data = train, method="lda")
#Confusion matrix with test data
lda.pred = predict(lda.fit,  data.test)
lda.class = lda.pred$class
table(lda.class,data.test$FRAUD_NONFRAUD)
sum(lda.class == data.test$FRAUD_NONFRAUD)/nrow(data.test)
#The probabilities for each class
lda.pred$posterior
head(lda.pred$posterior)
head(lda.pred$posterior[,'Non-Fraud'])
#ROC curve
test_roc = roc(as.factor(data.test$FRAUD_NONFRAUD) ~ lda.pred$posterior[,'Non-Fraud'], plot = TRUE, print.auc = TRUE)
#Obtain new cut off value
cutoff = test_roc$thresholds[test_roc$sensitivities>.6 & test_roc$specificities >.7]
range(cutoff)
#Number of predicted control (no dieases) for cut off probability .6
sum(lda.pred$posterior >.6)
pred2 = ifelse(lda.pred$posterior[,'Non-Fraud'] > .6, "Non-Fraud", "Fraud")
table(pred2,data.test$FRAUD_NONFRAUD)
sum(pred2 == test$FRAUD_NONFRAUD)/nrow(test)
#qda.fit = qda(FRAUD_NONFRAUD~ (TRAN_AMT) + log(ACCT_PRE_TRAN_AVAIL_BAL+1), data = train)
qda.fit = qda(FRAUD_NONFRAUD~ log(TRAN_AMT) + log(ACCT_PRE_TRAN_AVAIL_BAL+1), data = train)
qda.fit
partimat(as.factor(FRAUD_NONFRAUD)~ log(TRAN_AMT) + log(1+ACCT_PRE_TRAN_AVAIL_BAL), data = train, method="qda")
#Confusion matrix with test data
qda.pred = predict(qda.fit,  test)
qda.class = qda.pred$class
table(qda.class,test$FRAUD_NONFRAUD)
sum(qda.class == test$FRAUD_NONFRAUD)/nrow(test)
#The probabilities for each class
qda.pred$posterior
head(qda.pred$posterior)
head(qda.pred$posterior[,'Non-Fraud'])
#ROC
library(pROC)
test_roc = roc(as.factor(test$FRAUD_NONFRAUD) ~ qda.pred$posterior[,2],
plot = TRUE, print.auc = TRUE)
cutoff = test_roc$thresholds[test_roc$sensitivities>.7 & test_roc$specificities >.7]
range(cutoff)
#Number of predicted control (no dieases) for cut off probability .6
pred2 = ifelse(qda.pred$posterior[,'Non-Fraud'] > .2, "Non-Fraud", "Fraud")
table(pred2,test$FRAUD_NONFRAUD)
sum(pred2 == test$FRAUD_NONFRAUD)/nrow(test)
#Comparing two qda models
#Comparing two qda models
#Confusion matrix with test data
lda.pred = predict(lda.fit,  data.test)
lda.class = lda.pred$class
#Obtain new cut off value
cutoff = test_roc$thresholds[test_roc$sensitivities>.6 & test_roc$specificities >.7]
range(cutoff)
#ROC curve
test_roc = roc(as.factor(data.test$FRAUD_NONFRAUD) ~ lda.pred$posterior[,'Non-Fraud'], plot = TRUE, print.auc = TRUE)
table(pred2,data.test$FRAUD_NONFRAUD)
sum(pred2 == test$FRAUD_NONFRAUD)/nrow(test)
#Number of predicted control (no dieases) for cut off probability .6
sum(lda.pred$posterior >.6)
pred2 = ifelse(lda.pred$posterior[,'Non-Fraud'] > .6, "Non-Fraud", "Fraud")
table(pred2,data.test$FRAUD_NONFRAUD)
sum(pred2 == test$FRAUD_NONFRAUD)/nrow(test)
#qda.fit = qda(FRAUD_NONFRAUD~ (TRAN_AMT) + log(ACCT_PRE_TRAN_AVAIL_BAL+1), data = train)
qda.fit = qda(FRAUD_NONFRAUD~ log(TRAN_AMT) + log(ACCT_PRE_TRAN_AVAIL_BAL+1), data = train)
qda.fit
#The probabilities for each class
qda.pred$posterior
head(qda.pred$posterior)
head(qda.pred$posterior[,'Non-Fraud'])
head(qda.pred$posterior)
head(qda.pred$posterior[,'Non-Fraud'])
#ROC
library(pROC)
test_roc = roc(as.factor(test$FRAUD_NONFRAUD) ~ qda.pred$posterior[,2],
plot = TRUE, print.auc = TRUE)
cutoff = test_roc$thresholds[test_roc$sensitivities>.7 & test_roc$specificities >.7]
range(cutoff)
#Number of predicted control (no dieases) for cut off probability .6
# did not choose this for any reason that it was the lower boundary and
# had already tried up by .5
pred2 = ifelse(qda.pred$posterior[,'Non-Fraud'] > .2, "Non-Fraud", "Fraud")
table(pred2,test$FRAUD_NONFRAUD)
sum(pred2 == test$FRAUD_NONFRAUD)/nrow(test)
